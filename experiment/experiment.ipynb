{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09ed2de3",
   "metadata": {},
   "source": [
    "Project Structure\n",
    "- Setup Chunking and processing steps for inputs (PDFs, docx, txt)\n",
    "- Setup PGVector Store (VS) + postgresql => Done\n",
    "- Using LLMs (OpenAI, Gemini,...) to query vectors from VS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d89cd4e",
   "metadata": {},
   "source": [
    "# Create PostgreDB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "356d472c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv('/Users/longnv/Coding/rag_llama_index/deployment/.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2fc01b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "db_name = os.environ['POSTGRES_DB']\n",
    "host = \"localhost\"\n",
    "password = os.environ['POSTGRES_PASSWORD']\n",
    "port = \"5432\"\n",
    "user = os.environ['POSTGRES_USER']\n",
    "# conn = psycopg2.connect(connection_string)\n",
    "conn = psycopg2.connect(\n",
    "    dbname=db_name,\n",
    "    host=host,\n",
    "    password=password,\n",
    "    port=port,\n",
    "    user=user,\n",
    ")\n",
    "conn.autocommit = True\n",
    "\n",
    "# with conn.cursor() as c:\n",
    "#     c.execute(f\"DROP DATABASE IF EXISTS {db_name}\")\n",
    "#     c.execute(f\"CREATE DATABASE {db_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c0431d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "{'user': 'admin', 'channel_binding': 'prefer', 'dbname': 'rag_db', 'host': 'localhost', 'port': '5432', 'options': '', 'sslmode': 'prefer', 'sslcompression': '0', 'sslcertmode': 'allow', 'sslsni': '1', 'ssl_min_protocol_version': 'TLSv1.2', 'gssencmode': 'prefer', 'krbsrvname': 'postgres', 'gssdelegation': '0', 'target_session_attrs': 'any', 'load_balance_hosts': 'disable'}\n",
      "Connection is active\n"
     ]
    }
   ],
   "source": [
    "print(conn.status)\n",
    "print(conn.get_dsn_parameters())\n",
    "with conn.cursor() as cursor:\n",
    "    cursor.execute(\"SELECT 1\")\n",
    "    print(\"Connection is active\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f75f42d",
   "metadata": {},
   "source": [
    "# Checking file input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ca7925",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field, field_validator\n",
    "from pydantic_ai import Agent, RunContext\n",
    "from uuid import UUID\n",
    "from chonkie import TokenChunker, SemanticChunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1770fbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import PyPDF2\n",
    "from chonkie import SemanticChunker\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extract text from PDF file using PyPDF2.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted text from all pages\n",
    "    \"\"\"\n",
    "    text = \"\"\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "\n",
    "        for page_num in range(len(pdf_reader.pages)):\n",
    "            page = pdf_reader.pages[page_num]\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def chunk_with_semantic_chunker(text, chunk_size=512, similarity_threshold=0.7, embedding_model=None):\n",
    "    \"\"\"\n",
    "    Chunk text using Chonkie's SemanticChunker with custom embedding model.\n",
    "\n",
    "    Args:\n",
    "        text (str): Text to chunk\n",
    "        chunk_size (int): Maximum tokens per chunk\n",
    "        similarity_threshold (float): Similarity threshold for semantic chunking\n",
    "        embedding_model: Custom embedding model (SentenceTransformer or similar)\n",
    "\n",
    "    Returns:\n",
    "        list: List of chunks\n",
    "    \"\"\"\n",
    "    if embedding_model:\n",
    "        chunker = SemanticChunker(\n",
    "            chunk_size=chunk_size,\n",
    "            similarity_threshold=similarity_threshold,\n",
    "            embedding_model=embedding_model\n",
    "        )\n",
    "    else:\n",
    "        # Use default embedding model\n",
    "        chunker = SemanticChunker(\n",
    "            chunk_size=chunk_size,\n",
    "            similarity_threshold=similarity_threshold\n",
    "        )\n",
    "\n",
    "    chunks = chunker.chunk(text)\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def save_chunks_to_file(chunks, output_path, chunker_type):\n",
    "    \"\"\"\n",
    "    Save chunks to a text file.\n",
    "\n",
    "    Args:\n",
    "        chunks (list): List of chunks\n",
    "        output_path (str): Output file path\n",
    "        chunker_type (str): Type of chunker used\n",
    "    \"\"\"\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"Chunks created using {chunker_type}\\n\")\n",
    "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "\n",
    "        for i, chunk in enumerate(chunks, 1):\n",
    "            f.write(f\"Chunk {i}:\\n\")\n",
    "            f.write(\"-\" * 20 + \"\\n\")\n",
    "            f.write(f\"{chunk.text}\\n\\n\")\n",
    "            f.write(f\"Tokens: {chunk.token_count}\\n\")\n",
    "            if hasattr(chunk, 'start_index'):\n",
    "                f.write(f\"Start Index: {chunk.start_index}\\n\")\n",
    "            if hasattr(chunk, 'end_index'):\n",
    "                f.write(f\"End Index: {chunk.end_index}\\n\")\n",
    "            f.write(\"\\n\" + \"=\"*50 + \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aedf171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1: Loading documents\n",
      "\n",
      "Step 2: Loading embedding models...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd85f3cf86ee4d6284d3fc3bf0327132",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programs\\Anaconda\\envs\\longnv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ADMIN\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "143e987f93ec4ed383d0209d5aa24fff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c65950cf0527476eb051172f5e0bebc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b54ee53c2bbe4ff1b81932fbe051fa8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programs\\Anaconda\\envs\\longnv\\Lib\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6fb16e22ec34d3e805aff67d23848ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4246dbf1e394a1abdbcaa278ccef3f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86e89413bea34a168293fec6546836d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "995b33c6167e41ef8aa8899498f54495",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43d7b506e28b4036953fd48b4adfe79d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33cdcace29da418d9703f216b8075684",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f682a61a5ea4a94968d77e02e07ecce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 3: Chunking with SemanticChunker\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "embedding_model must be a string or a BaseEmbeddings object",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m embedding_model = SentenceTransformer(\u001b[33m'\u001b[39m\u001b[33mall-MiniLM-L6-v2\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mStep 3: Chunking with SemanticChunker\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m semantic_chunks = \u001b[43mchunk_with_semantic_chunker\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43msimilarity_threshold\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43membedding_model\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 43\u001b[39m, in \u001b[36mchunk_with_semantic_chunker\u001b[39m\u001b[34m(text, chunk_size, similarity_threshold, embedding_model)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[33;03mChunk text using Chonkie's SemanticChunker with custom embedding model.\u001b[39;00m\n\u001b[32m     32\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     40\u001b[39m \u001b[33;03m    list: List of chunks\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m embedding_model:\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m     chunker = \u001b[43mSemanticChunker\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m        \u001b[49m\u001b[43msimilarity_threshold\u001b[49m\u001b[43m=\u001b[49m\u001b[43msimilarity_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m        \u001b[49m\u001b[43membedding_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43membedding_model\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     49\u001b[39m     \u001b[38;5;66;03m# Use default embedding model\u001b[39;00m\n\u001b[32m     50\u001b[39m     chunker = SemanticChunker(\n\u001b[32m     51\u001b[39m         chunk_size=chunk_size,\n\u001b[32m     52\u001b[39m         similarity_threshold=similarity_threshold\n\u001b[32m     53\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Programs\\Anaconda\\envs\\longnv\\Lib\\site-packages\\chonkie\\chunker\\semantic.py:99\u001b[39m, in \u001b[36mSemanticChunker.__init__\u001b[39m\u001b[34m(self, embedding_model, threshold, chunk_size, similarity_window, min_sentences_per_chunk, min_characters_per_sentence, delim, include_delim, skip_window, filter_window, filter_polyorder, filter_tolerance, **kwargs)\u001b[39m\n\u001b[32m     97\u001b[39m     \u001b[38;5;28mself\u001b[39m.embedding_model = embedding_model\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33membedding_model must be a string or a BaseEmbeddings object\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    101\u001b[39m \u001b[38;5;66;03m# Lazy import dependencies\u001b[39;00m\n\u001b[32m    102\u001b[39m \u001b[38;5;28mself\u001b[39m._import_dependencies()\n",
      "\u001b[31mValueError\u001b[39m: embedding_model must be a string or a BaseEmbeddings object"
     ]
    }
   ],
   "source": [
    "print(\"\\nStep 1: Loading documents\")\n",
    "texts = extract_text_from_pdf(r'D:\\Project\\rag_with_llama\\docs\\llama2.pdf')\n",
    "\n",
    "# Create output directory\n",
    "output_dir = \"chunked_output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(\"\\nStep 2: Loading embedding models...\")\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "model_name = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "print(\"\\nStep 3: Chunking with SemanticChunker\")\n",
    "semantic_chunks = chunk_with_semantic_chunker(\n",
    "    texts,\n",
    "    chunk_size=512,\n",
    "    similarity_threshold=0.7,\n",
    "    embedding_model=embedding_model\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb4060f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055bedd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8615d83b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "longnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
