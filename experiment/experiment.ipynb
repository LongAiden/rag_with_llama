{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09ed2de3",
   "metadata": {},
   "source": [
    "Project Structure\n",
    "- Setup Chunking and processing steps for inputs (PDFs, docx, txt)\n",
    "- Setup PGVector Store (VS) + postgresql => Done\n",
    "- Using LLMs (OpenAI, Gemini,...) to query vectors from VS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d89cd4e",
   "metadata": {},
   "source": [
    "# Create PostgreDB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "356d472c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv('../deployment/.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2fc01b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "db_name = os.environ['POSTGRES_DB']\n",
    "host = \"localhost\"\n",
    "password = os.environ['POSTGRES_PASSWORD']\n",
    "port = \"5432\"\n",
    "user = os.environ['POSTGRES_USER']\n",
    "# conn = psycopg2.connect(connection_string)\n",
    "conn = psycopg2.connect(\n",
    "    dbname=db_name,\n",
    "    host=host,\n",
    "    password=password,\n",
    "    port=port,\n",
    "    user=user,\n",
    ")\n",
    "conn.autocommit = True\n",
    "\n",
    "# with conn.cursor() as c:\n",
    "#     c.execute(f\"DROP DATABASE IF EXISTS {db_name}\")\n",
    "#     c.execute(f\"CREATE DATABASE {db_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c0431d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "{'user': 'admin', 'channel_binding': 'prefer', 'dbname': 'rag_db', 'host': 'localhost', 'port': '5432', 'options': '', 'sslmode': 'prefer', 'sslcompression': '0', 'sslcertmode': 'allow', 'sslsni': '1', 'ssl_min_protocol_version': 'TLSv1.2', 'gssencmode': 'prefer', 'krbsrvname': 'postgres', 'gssdelegation': '0', 'target_session_attrs': 'any', 'load_balance_hosts': 'disable'}\n",
      "Connection is active\n"
     ]
    }
   ],
   "source": [
    "print(conn.status)\n",
    "print(conn.get_dsn_parameters())\n",
    "with conn.cursor() as cursor:\n",
    "    cursor.execute(\"SELECT 1\")\n",
    "    print(\"Connection is active\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddeb5fee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6fbefe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f75f42d",
   "metadata": {},
   "source": [
    "# Checking file input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08ca7925",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field, field_validator\n",
    "from pydantic_ai import Agent, RunContext\n",
    "from uuid import UUID\n",
    "from chonkie import TokenChunker, SemanticChunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1770fbc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniconda/base/envs/project/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "from pathlib import Path\n",
    "import PyPDF2\n",
    "from chonkie import SemanticChunker\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extract text from PDF file using PyPDF2.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted text from all pages\n",
    "    \"\"\"\n",
    "    text = \"\"\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "\n",
    "        for page_num in range(len(pdf_reader.pages)):\n",
    "            page = pdf_reader.pages[page_num]\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def chunk_with_semantic_chunker(text, chunk_size=512, similarity_threshold=0.7, embedding_model=None):\n",
    "    \"\"\"\n",
    "    Chunk text using Chonkie's SemanticChunker with custom embedding model.\n",
    "\n",
    "    Args:\n",
    "        text (str): Text to chunk\n",
    "        chunk_size (int): Maximum tokens per chunk\n",
    "        similarity_threshold (float): Similarity threshold for semantic chunking\n",
    "        embedding_model: Custom embedding model (SentenceTransformer or similar)\n",
    "\n",
    "    Returns:\n",
    "        list: List of chunks\n",
    "    \"\"\"\n",
    "    if embedding_model:\n",
    "        chunker = SemanticChunker(\n",
    "            chunk_size=chunk_size,\n",
    "            similarity_threshold=similarity_threshold,\n",
    "            embedding_model=embedding_model\n",
    "        )\n",
    "    else:\n",
    "        # Use default embedding model\n",
    "        chunker = SemanticChunker(\n",
    "            chunk_size=chunk_size,\n",
    "            similarity_threshold=similarity_threshold\n",
    "        )\n",
    "\n",
    "    chunks = chunker.chunk(text)\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def save_chunks_to_file(chunks, output_path, chunker_type):\n",
    "    \"\"\"\n",
    "    Save chunks to a text file.\n",
    "\n",
    "    Args:\n",
    "        chunks (list): List of chunks\n",
    "        output_path (str): Output file path\n",
    "        chunker_type (str): Type of chunker used\n",
    "    \"\"\"\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"Chunks created using {chunker_type}\\n\")\n",
    "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "\n",
    "        for i, chunk in enumerate(chunks, 1):\n",
    "            f.write(f\"Chunk {i}:\\n\")\n",
    "            f.write(\"-\" * 20 + \"\\n\")\n",
    "            f.write(f\"{chunk.text}\\n\\n\")\n",
    "            f.write(f\"Tokens: {chunk.token_count}\\n\")\n",
    "            if hasattr(chunk, 'start_index'):\n",
    "                f.write(f\"Start Index: {chunk.start_index}\\n\")\n",
    "            if hasattr(chunk, 'end_index'):\n",
    "                f.write(f\"End Index: {chunk.end_index}\\n\")\n",
    "            f.write(\"\\n\" + \"=\"*50 + \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5aedf171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1: Loading documents\n",
      "\n",
      "Step 2: Chunking with SemanticChunker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniconda/base/envs/project/lib/python3.12/site-packages/chonkie/embeddings/auto.py:87: UserWarning: Failed to load minishlab/potion-base-32M with Model2VecEmbeddings: model2vec is not available. Please install it via `pip install chonkie[model2vec]`\n",
      "Falling back to loading default provider model.\n",
      "  warnings.warn(\n",
      "/usr/local/Caskroom/miniconda/base/envs/project/lib/python3.12/site-packages/chonkie/embeddings/auto.py:95: UserWarning: Failed to load the default model for Model2VecEmbeddings: model2vec is not available. Please install it via `pip install chonkie[model2vec]`\n",
      "Falling back to SentenceTransformerEmbeddings.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nStep 1: Loading documents\")\n",
    "# texts = extract_text_from_pdf(r'D:\\Project\\rag_with_llama\\docs\\llama2.pdf')\n",
    "texts = extract_text_from_pdf(r'../docs/llama2.pdf')\n",
    "\n",
    "# Create output directory\n",
    "output_dir = \"chunked_output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(\"\\nStep 2: Chunking with SemanticChunker\")\n",
    "semantic_chunks = chunk_with_semantic_chunker(\n",
    "    texts,\n",
    "    chunk_size=512,\n",
    "    similarity_threshold=0.7\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5eb4060f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chunk(text='Llama 2 : Open Foundation and Fine-Tuned Chat Models\n",
       "Hugo Touvron∗Louis Martin†Kevin Stone†\n",
       "Peter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra\n",
       "', token_count=43, start_index=0, end_index=167)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5448515",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "class EmbeddingGenerator:\n",
    "    \"\"\"Generate embeddings using SentenceTransformers.\"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):\n",
    "        \"\"\"\n",
    "        Initialize embedding generator.\n",
    "\n",
    "        Args:\n",
    "            model_name: SentenceTransformer model name\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.embedding_dim = self.model.get_sentence_embedding_dimension()\n",
    "\n",
    "    def embed_text(self, text: str) -> List[float]:\n",
    "        \"\"\"\n",
    "        Generate embedding for a single text.\n",
    "        Args:\n",
    "            text: Input text\n",
    "        Returns:\n",
    "            List of embedding values\n",
    "        \"\"\"\n",
    "        embedding = self.model.encode(text)\n",
    "        return embedding.tolist()\n",
    "\n",
    "    def embed_batch(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"\n",
    "        Generate embeddings for multiple texts.\n",
    "        Args:\n",
    "            texts: List of input texts\n",
    "        Returns:\n",
    "            List of embedding lists\n",
    "        \"\"\"\n",
    "        embeddings = self.model.encode(texts)\n",
    "        return [emb.tolist() for emb in embeddings]\n",
    "\n",
    "embedding_model = './embedded_model/all-MiniLM-L6-v2'\n",
    "embedding_generator = EmbeddingGenerator(embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "132653c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_txt = embedding_generator.embed_batch(semantic_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33e004a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1332"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedded_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e38b2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from pgvector.psycopg2 import register_vector  # Missing import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dde964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- First, turn off the pager to avoid the 'more' error\n",
    "# \\pset pager off\n",
    "\n",
    "# -- Now run your queries (notice the semicolon at the end)\n",
    "# SELECT version();\n",
    "\n",
    "# SELECT 2+2 AS result;\n",
    "\n",
    "# SELECT NOW();\n",
    "\n",
    "# SELECT 'Hello PostgreSQL!' AS message;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f89015a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sentence_transformers\n",
    "# model = sentence_transformers.SentenceTransformer('all-MiniLM-L6-v2')\n",
    "# model.save('./embedded_model/all-MiniLM-L6-v2')\n",
    "# model_test = sentence_transformers.SentenceTransformer('./embedded_model/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb580acd",
   "metadata": {},
   "source": [
    "# Test fulll flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2fa63fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddc6f726",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/longnv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "from fastapi import FastAPI, File, UploadFile, HTTPException, Form\n",
    "from fastapi.responses import HTMLResponse\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "import sys\n",
    "sys.path.append('../document_processing')  # Go up one level, then into folder\n",
    "\n",
    "import sys\n",
    "sys.path.append('../docs')  # Go up one level, then into folder\n",
    "\n",
    "# Your existing components - unchanged\n",
    "from embed_chunks_to_db import ChunkEmbeddingPipeline, EmbeddingGenerator, VectorStore\n",
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8266c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_params = {\n",
    "    'host': os.getenv('DB_HOST', 'localhost'),\n",
    "    'port': os.getenv('DB_PORT', '5432'),\n",
    "    'dbname': os.getenv('DB_NAME', 'rag_db'),\n",
    "    'user': os.getenv('DB_USER', 'admin'),\n",
    "    'password': os.getenv('DB_PASSWORD', 'admin')\n",
    "}\n",
    "\n",
    "table_name = 'document_chunks'\n",
    "\n",
    "# Don't initialize models at startup\n",
    "pipeline = None\n",
    "def get_pipeline():\n",
    "    global pipeline\n",
    "    if pipeline is None:\n",
    "        pipeline = ChunkEmbeddingPipeline(\n",
    "                    db_params=db_params,\n",
    "                    embedding_model='all-MiniLM-L6-v2',\n",
    "                    table_name=table_name\n",
    "                )\n",
    "    return pipeline\n",
    "\n",
    "pipeline = get_pipeline()\n",
    "file = pipeline.extract_text_from_pdf(r'../docs/llama2.pdf')\n",
    "chunks = pipeline.chunk_text(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62b751d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Vector type working: [1. 2. 3.]\n",
      "✓ Cosine distance: 1.000\n",
      "✓ Embedding column: ('embedding', 'USER-DEFINED')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_pgvector_connection():\n",
    "    \"\"\"Validate pgvector integration at the connection level\"\"\"\n",
    "    vector_store = VectorStore(db_params, 'document_chunks')\n",
    "    \n",
    "    try:\n",
    "        conn = vector_store._get_connection()\n",
    "        with conn.cursor() as cur:\n",
    "            # Test 1: Confirm pgvector types are registered\n",
    "            cur.execute(\"SELECT '[1,2,3]'::vector;\")\n",
    "            result = cur.fetchone()[0]\n",
    "            print(f\"✓ Vector type working: {result}\")\n",
    "            \n",
    "            # Test 2: Verify cosine distance operator\n",
    "            cur.execute(\"SELECT '[1,0,0]'::vector <=> '[0,1,0]'::vector;\")\n",
    "            distance = cur.fetchone()[0]\n",
    "            print(f\"✓ Cosine distance: {distance:.3f}\")\n",
    "            \n",
    "            # Test 3: Table schema validation\n",
    "            cur.execute(f\"\"\"\n",
    "                SELECT column_name, data_type \n",
    "                FROM information_schema.columns \n",
    "                WHERE table_name = '{vector_store.table_name}' \n",
    "                AND column_name = 'embedding'\n",
    "            \"\"\")\n",
    "            schema = cur.fetchone()\n",
    "            print(f\"✓ Embedding column: {schema}\")\n",
    "            \n",
    "        conn.close()\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ pgvector connection failed: {e}\")\n",
    "        return False\n",
    "    \n",
    "test_pgvector_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8291d995",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Chunk:\n",
    "    \"\"\"Chunk data structure to match your existing interface.\"\"\"\n",
    "    id: str\n",
    "    document_id: str\n",
    "    text: str\n",
    "    embedding: List[float]\n",
    "    metadata: Optional[Dict] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "232fc845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings...\n"
     ]
    }
   ],
   "source": [
    "# Generate embeddings in batch\n",
    "print(\"Generating embeddings...\")\n",
    "embeddings = pipeline.embedding_generator.embed_batch([chunk.text for chunk in chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac37546d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 512\n",
    "similarity_threshold = 0.7\n",
    "filename = 'llama2.pdf'\n",
    "file_type = 'pdf'\n",
    "file_size = len(file)\n",
    "document_id = str(uuid.uuid4())\n",
    "metadata = {'source': 'llama2.pdf'}\n",
    "\n",
    "# Create Chunk objects using your interface\n",
    "chunk_objects = []\n",
    "for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):\n",
    "    chunk_metadata = {\n",
    "        'chunk_index': i,\n",
    "        'token_count': chunk.token_count,\n",
    "        'start_index': getattr(chunk, 'start_index', None),\n",
    "        'end_index': getattr(chunk, 'end_index', None),\n",
    "        'chunk_size': chunk_size,\n",
    "        'similarity_threshold': similarity_threshold,\n",
    "        'embedding_model': pipeline.embedding_generator.model_name,\n",
    "        'embedding_dimension': len(embedding),\n",
    "        'filename': filename,\n",
    "        'file_type': file_type,\n",
    "        'file_size': file_size\n",
    "    }\n",
    "\n",
    "    # Add any additional metadata\n",
    "    if metadata:\n",
    "        chunk_metadata.update(metadata)\n",
    "\n",
    "    chunk_obj = Chunk(\n",
    "        id=str(uuid.uuid4()),\n",
    "        document_id=document_id,\n",
    "        text=chunk.text,\n",
    "        embedding=embedding,\n",
    "        metadata=chunk_metadata\n",
    "    )\n",
    "    chunk_objects.append(chunk_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31b78593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserting chunks into database using pgvector...\n"
     ]
    }
   ],
   "source": [
    "# Use your add_chunks method with pgvector\n",
    "print(\"Inserting chunks into database using pgvector...\")\n",
    "pipeline.vector_store.add_chunks(chunk_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ee851e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Search operational: 1 results\n"
     ]
    }
   ],
   "source": [
    "test_results = pipeline.search_documents('text', limit=1, threshold=0.1)\n",
    "assert len(test_results) > 0, \"Insert succeeded but search failed\"\n",
    "print(f\"✓ Search operational: {len(test_results)} results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdb0aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Data truncated instantly\n",
      "✓ Table schema dropped\n"
     ]
    }
   ],
   "source": [
    "def fast_reset_vector_store(vector_store):\n",
    "    \"\"\"Two-step reset: TRUNCATE then DROP for optimal performance\"\"\"\n",
    "    conn = vector_store._get_connection()\n",
    "    \n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            # Step 1: Instant data removal (no WAL overhead)\n",
    "            cur.execute(f\"TRUNCATE TABLE {vector_store.table_name} CASCADE;\")\n",
    "            print(\"✓ Data truncated instantly\")\n",
    "            \n",
    "            # Step 2: Clean schema removal\n",
    "            cur.execute(f\"DROP TABLE {vector_store.table_name} CASCADE;\")\n",
    "            print(\"✓ Table schema dropped\")\n",
    "            \n",
    "        conn.commit()\n",
    "        \n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        raise e\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "# # Usage\n",
    "# fast_reset_vector_store(pipeline.vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9569d22d",
   "metadata": {},
   "source": [
    "# Check inserting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b8653e2e",
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefinedTable",
     "evalue": "relation \"document_chunks\" does not exist\nLINE 1: SELECT COUNT(*) FROM document_chunks\n                             ^\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUndefinedTable\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m conn = pipeline.vector_store._get_connection()\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m conn.cursor() \u001b[38;5;28;01mas\u001b[39;00m cur:\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     \u001b[43mcur\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSELECT COUNT(*) FROM \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mpipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvector_store\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtable_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m     count_after = cur.fetchone()[\u001b[32m0\u001b[39m]\n\u001b[32m      7\u001b[39m     \u001b[38;5;66;03m# Validate embedding integrity\u001b[39;00m\n",
      "\u001b[31mUndefinedTable\u001b[39m: relation \"document_chunks\" does not exist\nLINE 1: SELECT COUNT(*) FROM document_chunks\n                             ^\n"
     ]
    }
   ],
   "source": [
    "# Verify success\n",
    "conn = pipeline.vector_store._get_connection()\n",
    "with conn.cursor() as cur:\n",
    "    cur.execute(f\"SELECT COUNT(*) FROM {pipeline.vector_store.table_name}\")\n",
    "    count_after = cur.fetchone()[0]\n",
    "    \n",
    "    # Validate embedding integrity\n",
    "    cur.execute(f\"\"\"\n",
    "        SELECT count(*)\n",
    "        FROM {pipeline.vector_store.table_name} \n",
    "    \"\"\")\n",
    "    \n",
    "    results = cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3d4e4fbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2740"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b800dad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('0cef73c0-a5ea-4b45-93f7-152110c773a0',\n",
       " '62fea6ed-85a6-464f-b0fd-5ec2402d0dd8',\n",
       " 'Prajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen\\n',\n",
       " array([ 7.12140650e-03,  1.58904716e-01, -4.11183424e-02, -1.60896499e-02,\n",
       "        -1.19244307e-01,  1.66793726e-02,  7.66922235e-02,  2.09896639e-03,\n",
       "        -7.99249765e-03, -3.11004985e-02,  6.69134455e-03, -1.19025603e-01,\n",
       "         9.55456793e-02, -4.00688797e-02,  2.43884884e-03,  2.39558239e-02,\n",
       "        -2.64738058e-03,  2.02801265e-02,  7.83016905e-03, -7.15690851e-02,\n",
       "        -2.38060020e-02, -1.06638998e-01, -9.48443916e-03,  2.73222420e-02,\n",
       "        -6.30895793e-02, -4.59778449e-03,  1.97286680e-02,  1.22988073e-03,\n",
       "         2.02850271e-02, -3.98318060e-02, -3.29108462e-02,  1.30037352e-01,\n",
       "         7.59432688e-02, -2.78772581e-02, -1.24753239e-02,  8.16547722e-02,\n",
       "        -5.94117120e-02,  4.51860111e-03,  1.69892292e-02, -4.73478902e-03,\n",
       "        -2.29553226e-02,  7.49923289e-03, -2.45867502e-02, -6.85824007e-02,\n",
       "         4.61076982e-02, -7.90486857e-03,  3.45357135e-02,  4.29701656e-02,\n",
       "        -1.81527287e-02,  3.94386984e-03, -1.12031281e-01, -1.33849233e-02,\n",
       "         2.95562968e-02, -5.68636507e-02, -1.91135257e-02, -4.89593633e-02,\n",
       "        -2.09067129e-02,  2.54858583e-02,  2.22643632e-02, -7.60168508e-02,\n",
       "         6.34239987e-02,  4.02081087e-02, -7.23926499e-02, -2.93469126e-03,\n",
       "        -6.95489207e-03, -1.99586619e-02, -5.47327213e-02, -6.26659393e-02,\n",
       "        -5.50053380e-02, -3.49979922e-02,  5.01937158e-02, -4.63738479e-02,\n",
       "        -5.19338474e-02, -2.70534437e-02, -4.35601696e-02, -4.47141044e-02,\n",
       "         1.51812294e-02, -2.63850186e-02, -5.31597435e-02, -5.92830963e-02,\n",
       "        -8.20158124e-02, -3.04792225e-02,  2.42744926e-02,  5.42199053e-02,\n",
       "        -2.90954467e-02,  4.95423526e-02, -7.87444320e-03, -3.05384248e-02,\n",
       "        -2.47603059e-02, -3.60754542e-02,  9.20121837e-03,  4.02520634e-02,\n",
       "        -2.28688978e-02,  4.39353213e-02, -7.07207397e-02,  3.69134136e-02,\n",
       "         2.79467343e-03,  2.21682582e-02,  1.65243316e-02,  9.99063402e-02,\n",
       "         2.19089799e-02,  2.90145334e-02,  6.05171099e-02,  1.35710761e-02,\n",
       "        -1.58481196e-01, -4.70414683e-02,  4.13059294e-02,  6.38688728e-02,\n",
       "         1.91920176e-02,  1.73744671e-02, -3.81577015e-02,  7.30553409e-03,\n",
       "        -3.25665027e-02, -1.01912543e-01, -4.59164493e-02, -1.08445007e-02,\n",
       "        -2.01329105e-02,  4.14437801e-02, -3.18212733e-02,  2.55878437e-02,\n",
       "         3.90040651e-02,  5.42499535e-02, -6.37429953e-02, -6.79228976e-02,\n",
       "         1.62571687e-02, -2.59845518e-02,  5.26357926e-02,  1.99033851e-32,\n",
       "        -4.95871827e-02, -7.90225193e-02,  9.21913460e-02,  3.56467366e-02,\n",
       "         4.26651491e-03, -5.04414849e-02,  5.72552858e-03, -8.60541537e-02,\n",
       "        -2.67258249e-02, -3.08261216e-02,  4.47884426e-02, -3.62987556e-02,\n",
       "         1.18823377e-02, -5.44895008e-02,  1.08716570e-01,  4.03876938e-02,\n",
       "        -2.28537563e-02, -9.44421589e-02, -1.51066873e-02, -4.98357869e-04,\n",
       "         3.80847156e-02,  5.21642007e-02,  4.60770875e-02,  1.45992888e-02,\n",
       "         3.70202474e-02,  5.84117696e-02,  1.20251216e-01, -1.98231377e-02,\n",
       "         6.56113401e-03,  5.06151505e-02,  8.78756344e-02, -2.27499567e-02,\n",
       "        -8.11563209e-02, -4.98249568e-02, -2.63267681e-02, -2.03689863e-03,\n",
       "        -5.87149225e-02, -1.28698833e-02, -5.34305014e-02, -4.73356843e-02,\n",
       "         2.73009297e-02, -7.30978101e-02,  2.87801493e-02, -3.14410143e-02,\n",
       "         2.24582478e-02,  4.58435714e-02,  7.48047084e-02,  6.37950152e-02,\n",
       "         1.31521095e-03,  2.74903048e-02, -2.85092257e-02,  4.53276187e-02,\n",
       "         5.38056670e-03,  4.09435779e-02,  1.66451894e-02, -6.75904378e-02,\n",
       "         2.06154920e-02,  2.42883749e-02, -7.47049153e-02,  8.88601616e-02,\n",
       "         2.31793839e-02,  7.47747570e-02, -3.27695273e-02,  9.43940952e-02,\n",
       "        -3.37526500e-02, -5.39264716e-02,  6.41731312e-03, -6.64855465e-02,\n",
       "         4.43784818e-02, -6.38131946e-02, -4.49763089e-02, -2.10196301e-02,\n",
       "         3.02143265e-02,  7.05433413e-02, -8.37858170e-02, -3.68047156e-04,\n",
       "        -6.19669259e-02, -5.90805225e-02, -7.67595470e-02,  4.91671637e-02,\n",
       "        -5.38995415e-02,  1.21844327e-02,  2.39255894e-02, -6.34106770e-02,\n",
       "        -2.19259271e-03, -1.16060358e-02,  2.53959242e-02, -5.77127337e-02,\n",
       "         2.50133034e-02,  7.29051381e-02, -6.13422617e-02, -2.86903307e-02,\n",
       "         2.06024689e-03,  8.79237205e-02,  1.19339358e-02, -1.92842008e-32,\n",
       "        -1.13939233e-02, -3.50413620e-02,  4.19868790e-02,  1.17696486e-02,\n",
       "         5.23530468e-02, -3.43589000e-02,  2.73568481e-02,  2.08445359e-02,\n",
       "        -1.03410035e-02, -1.68308243e-02, -4.45846245e-02,  1.05669284e-02,\n",
       "         6.18973821e-02,  9.44936275e-02, -6.17756918e-02,  5.24566844e-02,\n",
       "         1.13191418e-01,  2.86060628e-02, -8.47160146e-02, -1.62532330e-02,\n",
       "        -1.61346719e-02,  1.01161949e-01, -4.08219993e-02, -4.21427600e-02,\n",
       "        -4.29398455e-02,  6.21785931e-02,  1.71954501e-02,  2.37833411e-02,\n",
       "        -8.59973282e-02,  3.06853801e-02, -2.81311404e-02, -8.78163874e-02,\n",
       "        -1.22775529e-02,  9.63528156e-02, -2.23200899e-02, -4.50367145e-02,\n",
       "        -1.84228476e-02, -2.56144553e-02,  4.13243771e-02,  4.85375635e-02,\n",
       "        -2.49409564e-02, -4.53940816e-02,  2.64466405e-02,  1.75824650e-02,\n",
       "         6.50975630e-02, -9.91816446e-02, -4.42860499e-02,  1.61654260e-02,\n",
       "        -4.54406142e-02, -7.05770329e-02,  4.69271913e-02,  6.32627215e-03,\n",
       "         3.21154632e-02, -6.69564605e-02,  1.12510085e-01, -1.71790319e-03,\n",
       "        -4.10111845e-02, -8.68534262e-05,  6.95137009e-02, -3.34252529e-02,\n",
       "         3.40690017e-02,  7.38593265e-02,  3.15376022e-03,  1.19064949e-01,\n",
       "        -2.26225890e-02, -1.11520989e-02, -1.54929496e-02, -3.68549898e-02,\n",
       "         2.73986235e-02, -8.25179294e-02, -2.12857258e-02,  7.30377808e-03,\n",
       "        -6.67885691e-02,  2.31057964e-02, -7.47078881e-02,  6.34249523e-02,\n",
       "        -4.12976891e-02,  1.27696157e-01, -4.27455455e-03,  2.98922993e-02,\n",
       "         2.51764082e-03, -5.64641729e-02,  5.97488042e-03,  4.75125089e-02,\n",
       "        -1.72904544e-02, -1.18521350e-02, -3.83832073e-03, -1.04047880e-01,\n",
       "         1.17749706e-01, -3.12857255e-02, -8.90422147e-03,  4.12782244e-02,\n",
       "         1.18399356e-02,  2.44954303e-02,  3.10189780e-02, -5.12779614e-08,\n",
       "        -1.94626441e-03, -6.47577122e-02, -5.81220277e-02,  3.22320499e-02,\n",
       "        -1.14133628e-02, -2.08469573e-02,  3.36900689e-02, -6.17663786e-02,\n",
       "        -5.94899654e-02,  4.93952930e-02,  3.38843018e-02, -3.02643161e-02,\n",
       "         3.24728675e-02,  2.92481910e-02,  1.13855964e-02, -1.49579439e-02,\n",
       "         2.00547930e-02,  4.27083522e-02, -3.22103128e-02, -5.64295091e-02,\n",
       "         7.32141212e-02, -9.72100496e-02,  9.48110688e-03,  5.27855679e-02,\n",
       "        -2.25006286e-02, -5.36900721e-02, -2.66243760e-02, -1.13785937e-01,\n",
       "         2.67658476e-02, -1.76870394e-02,  1.28254360e-02,  2.57339031e-02,\n",
       "         8.35480765e-02, -6.82847127e-02,  6.94068074e-02,  8.35433602e-02,\n",
       "        -3.03503592e-02,  5.24762273e-02,  3.37132660e-04,  7.99124315e-02,\n",
       "        -1.21218078e-02, -3.97064425e-02,  1.11654855e-01,  1.36508849e-02,\n",
       "         1.82447862e-02, -5.93780093e-02, -4.88811061e-02, -3.04132774e-02,\n",
       "         6.38435408e-02, -5.49845882e-02, -2.99757719e-02, -6.15019165e-02,\n",
       "         6.99507445e-02,  3.23906988e-02, -3.30509171e-02,  8.95861760e-02,\n",
       "        -3.65244225e-02,  7.89555348e-03, -3.25192325e-02,  1.28854932e-02,\n",
       "         3.46299820e-02, -3.92249003e-02, -6.87561408e-02, -2.14037225e-02],\n",
       "       dtype=float32),\n",
       " {'source': 'llama2.pdf',\n",
       "  'filename': 'llama2.pdf',\n",
       "  'end_index': 257,\n",
       "  'file_size': 252376,\n",
       "  'file_type': 'pdf',\n",
       "  'chunk_size': 512,\n",
       "  'chunk_index': 1,\n",
       "  'start_index': 167,\n",
       "  'token_count': 29,\n",
       "  'embedding_model': 'all-MiniLM-L6-v2',\n",
       "  'embedding_dimension': 384,\n",
       "  'similarity_threshold': 0.7},\n",
       " datetime.datetime(2025, 9, 24, 23, 49, 39, 208998, tzinfo=datetime.timezone(datetime.timedelta(seconds=25200))))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d8e672f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv('./deployment/.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1f5fa406",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1758732769.428342 1368407 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    }
   ],
   "source": [
    "# embedding_model = './embedded_model/all-MiniLM-L6-v2'\n",
    "# embedding_generator = EmbeddingGenerator(embedding_model)\n",
    "query = \"What is Llama 2?\"\n",
    "results = pipeline.search_documents(\n",
    "    query=query,\n",
    "    limit=100,\n",
    "    threshold=0.5\n",
    ")\n",
    "\n",
    "# Step 2: Build context for LLM\n",
    "context = \"\\n\\n\".join([f\"[Context {i+1}]: {r['text']}\" for i, r in enumerate(results)])\n",
    "\n",
    "# Step 3: Generate response with Gemini (if available)\n",
    "gemini_key = os.getenv('GOOGLE_API_KEY')\n",
    "genai.configure(api_key=gemini_key)\n",
    "if gemini_key:\n",
    "    model = genai.GenerativeModel('gemini-2.5-flash')\n",
    "    prompt = f\"\"\"Answer based on this context:\n",
    "            {context}\n",
    "            Question: {query}\n",
    "            Answer:\"\"\"\n",
    "    \n",
    "    response = model.generate_content(prompt)\n",
    "    answer = response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6de7646f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Llama 2 is an updated version of Llama 1, trained on a new mix of publicly available data. It is a new technology and a Large Language Model (LLM) that was pretrained on 2 trillion tokens of data from publicly available sources. Llama 2 carries potential risks with its use.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3652862",
   "metadata": {},
   "source": [
    "# Test with Pydantic AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d6b2a98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv('../deployment/.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc1d8c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai import Agent\n",
    "from pydantic_ai.models.google import GoogleModel\n",
    "from pydantic_ai.providers.google import GoogleProvider\n",
    "\n",
    "provider = GoogleProvider(api_key=os.getenv('GOOGLE_API_KEY'))\n",
    "model = GoogleModel('gemini-2.5-flash', provider=provider)\n",
    "agent = Agent(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce24ff99",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'What is NLTK in context of text processing?'\n",
    "answer = await agent.run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f842b78f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentRunResult(output='**NLTK (Natural Language Toolkit)** is a powerful, open-source library written in **Python** that is widely used for working with human language data. In the context of text processing, it serves as a comprehensive platform for building Python programs to analyze, process, and understand natural language.\\n\\nThink of it as a Swiss Army knife for Natural Language Processing (NLP) tasks, especially for educational purposes, research, and rapid prototyping.\\n\\nHere\\'s a breakdown of what NLTK is and its role in text processing:\\n\\n### What is NLTK?\\n\\n*   **Python Library:** It\\'s a collection of modules and functions for Python.\\n*   **NLP Focus:** Specifically designed for tasks related to Natural Language Processing, which is a subfield of artificial intelligence that deals with the interaction between computers and human (natural) languages.\\n*   **Comprehensive:** It provides a vast array of algorithms and data sets (corpora) that support almost all facets of NLP.\\n*   **Educational & Research Tool:** It\\'s excellent for learning NLP concepts and for academic research due to its clear implementation of various algorithms and extensive documentation (including the famous \"NLTK Book\").\\n\\n### NLTK\\'s Role in Text Processing\\n\\nNLTK provides tools for many common and advanced text processing tasks, allowing you to manipulate, clean, analyze, and extract information from raw text. Key functionalities include:\\n\\n1.  **Tokenization:**\\n    *   **Purpose:** Breaking down text into smaller units (tokens). This is often the very first step in text processing.\\n    *   **Examples:**\\n        *   **Word Tokenization:** Splitting a sentence into individual words.\\n        *   **Sentence Tokenization:** Splitting a paragraph into individual sentences.\\n\\n2.  **Text Cleaning and Normalization:**\\n    *   **Stemming:** Reducing words to their root or base form (e.g., \"running,\" \"runs,\" \"ran\" -> \"run\"). NLTK includes stemmers like PorterStemmer and SnowballStemmer.\\n    *   **Lemmatization:** A more sophisticated approach than stemming, which reduces words to their meaningful base form (lemma) using a vocabulary and morphological analysis (e.g., \"better\" -> \"good\", \"am,\" \"are,\" \"is\" -> \"be\"). NLTK uses WordNet for lemmatization.\\n\\n3.  **Part-of-Speech (POS) Tagging:**\\n    *   **Purpose:** Identifying the grammatical category of each word in a sentence (e.g., noun, verb, adjective, adverb).\\n    *   **Example:** \"The (DT) quick (JJ) brown (JJ) fox (NN) jumps (VBZ) over (IN) the (DT) lazy (JJ) dog (NN).\" (DT=Determiner, JJ=Adjective, NN=Noun, VBZ=Verb third-person singular present, IN=Preposition).\\n\\n4.  **Named Entity Recognition (NER):**\\n    *   **Purpose:** Identifying and classifying named entities in text into predefined categories like person names, organizations, locations, dates, etc.\\n    *   **Example:** \"Barack Obama (PERSON) visited New York (LOCATION) on Tuesday (DATE).\"\\n\\n5.  **Parsing:**\\n    *   **Purpose:** Analyzing the grammatical structure of sentences (syntax) to understand the relationships between words.\\n    *   **Examples:** NLTK supports various parsers, including constituency parsing (building parse trees) and dependency parsing.\\n\\n6.  **Text Classification:**\\n    *   **Purpose:** Categorizing documents or text snippets into predefined classes (e.g., spam detection, sentiment analysis, topic classification).\\n    *   **Tools:** NLTK provides utilities to build and evaluate classifiers using various machine learning algorithms.\\n\\n7.  **Access to Corpora and Lexical Resources:**\\n    *   **Corpora:** NLTK comes with a large collection of curated text data (e.g., the Brown Corpus, Gutenberg Corpus, Reuters Corpus, Penn Treebank) which are invaluable for training models, performing linguistic analysis, and testing algorithms.\\n    *   **WordNet:** A lexical database for the English language that groups words into sets of synonyms called \"synsets,\" and provides short definitions and usage examples, along with various semantic relations.\\n\\n8.  **Frequency Distributions and Collocations:**\\n    *   **Frequency:** Counting word occurrences to understand the most common words in a text.\\n    *   **Collocations:** Identifying sequences of words that frequently occur together (e.g., \"New York,\" \"strong tea\").\\n\\n### Simple Example: Tokenization and POS Tagging\\n\\n```python\\nimport nltk\\n# Download necessary data (do this once)\\n# nltk.download(\\'punkt\\')\\n# nltk.download(\\'averaged_perceptron_tagger\\')\\n\\ntext = \"NLTK is a powerful library for natural language processing.\"\\n\\n# 1. Word Tokenization\\nwords = nltk.word_tokenize(text)\\nprint(\"Words:\", words)\\n# Output: Words: [\\'NLTK\\', \\'is\\', \\'a\\', \\'powerful\\', \\'library\\', \\'for\\', \\'natural\\', \\'language\\', \\'processing\\', \\'.\\']\\n\\n# 2. Part-of-Speech Tagging\\npos_tags = nltk.pos_tag(words)\\nprint(\"POS Tags:\", pos_tags)\\n# Output: POS Tags: [(\\'NLTK\\', \\'NNP\\'), (\\'is\\', \\'VBZ\\'), (\\'a\\', \\'DT\\'), (\\'powerful\\', \\'JJ\\'), (\\'library\\', \\'NN\\'), (\\'for\\', \\'IN\\'), (\\'natural\\', \\'JJ\\'), (\\'language\\', \\'NN\\'), (\\'processing\\', \\'NN\\'), (\\'.\\', \\'.\\')]\\n```\\n\\n### Conclusion\\n\\nIn essence, NLTK empowers developers, researchers, and students to perform a wide range of text processing tasks, from basic text cleanup and structural analysis to more advanced classification and semantic understanding, all within the Python ecosystem. While more performance-optimized libraries like spaCy have emerged for production NLP, NLTK remains an invaluable tool for learning, experimentation, and research due to its extensive features and educational focus.')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b9f34a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "longnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
